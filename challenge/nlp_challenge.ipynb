{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge - Clinical Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You now know the structure of a Machine Learning pipeline, and how to implement it from the pre-processing of data to the evaluation of results. Good job! But can you do it with text data? In this challenge, you will have the opportunity to implement text processing techniques, including a simple strategy to represent text data in a structure that is compatible with Machine Learning algorithms: the Bag of Words model. Leveraging this strategy, and combining it with what you have learned by implementing the previous pipeline, you will be able to classify clinical notes according to the medical specialty where they were produced. Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "!pip install scikit-learn\n",
    "!pip install nltk\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import feature_extraction\n",
    "from sklearn.metrics import balanced_accuracy_score, f1_score\n",
    "from collections import Counter\n",
    "from random import randint\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"nlp_challenge.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the data look? Here are a few examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"description\" columns contains the samples of the data you will use to train your model. The target is the \"medical_specialty\" column. As you can see, these data is very different from time series! Start by isolating the information you need (i.e., create the variables X_data and y_data, referring to the description and medical_specialty columns, respectively)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data, y_data = data[\"description\"], data[\"medical_specialty\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, notice that your data is not split into train and test sets. Splitting data is one of the most defining steps in the Machine Learning pipeline. A bad split could result in data leakage, or a test set that does not accurately represent the true data distribution, which could consequently lead to over or under-estimation of results.\n",
    "\n",
    "To avoid falling into this pits, one must know the data one's working with. Let us check the class distributions. To do so, we will make use of the Counter from the collections library, which gives us the occurrences of each unique instance within a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to make sure this distribution is similar both in training and testing. Apply a stratified method to do the split (https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idxs, test_idxs = next(StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=32).split(X_data, y_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = X_data[train_idxs], y_data[train_idxs], X_data[test_idxs], y_data[test_idxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the class distributions on both resultant sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(y_train) # Add code to obtain the result in terms of percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(y_test) # Add code to obtain the result in terms of percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get some words rolling! Here's a tutorial of text processing techniques: https://towardsdatascience.com/text-preprocessing-in-natural-language-processing-using-python-6113ff5decd8\n",
    "You should complement the function \"preprocess_text\", to return a list of clean tokens, when receiving a chunk of text. Note that the output of this function must be tokenized, to be compatible with the following steps of the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"Takes a text and returns the processed version of it.\n",
    "\n",
    "    Args:\n",
    "        text (str): raw text\n",
    "\n",
    "    Returns:\n",
    "        list: set of clean tokens containing the content of text\n",
    "    \"\"\"\n",
    "\n",
    "    # Normalize Text (lowercasing)\n",
    "    processed_text = text.lower()\n",
    "\n",
    "    # Add more preprocessing steps below:\n",
    "\n",
    "    # Try a few preprocessing techniques:\n",
    "    # - Tokenization                       <---- Essential\n",
    "    # - Stopwords Removal\n",
    "    # - Stemming/Lemmatization\n",
    "\n",
    "    # You can also implement other preprocessing you may find useful\n",
    "\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the function on a randomly sampled tweet from the train dataset (Give it a couple of tries to really see impact)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_description = X_train.values[randint(0, X_train.shape[0])]\n",
    "print(f\"Original Note: {random_description}\")\n",
    "print(f\"Processed Note: {preprocess_text(random_description)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have cleaned every entry in our dataset, you can proceed to extract features. There are many ways to do this, but we will focus on two simple techniques: The bag-of-words model, and the tfidf. In the following cells, you'll be able to try several these two ways of vectorizing the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine Learning models do not accept strings as input. So how can we train one to perform text classification? We need to represent each text sample into a numeric vector. This numeric vector is our feature vector. In this challenge, you will implement two simple methods for text vectorization: The bag-of-words and the tfidf methodologies. Here's a quick introduction to both of them: https://machinelearningmastery.com/gentle-introduction-bag-words-model/. These functions are already implemented. However, you must learn the processes behind these implementations, according to this article. So read it carefully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag-of-Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement a bag-of-words vectorization, we will use the CountVectorizer function from sklearn (https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Note: Notice that this function expects the output from the preprocessing function to be a tokenized tweet. If you did not implement a tokenizer yet, you must re-think your preprocessing methodology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bow_representations(train_samples, test_samples, tokenizer):\n",
    "    \"\"\"Returns a bag-of-words based representation of both the train and test samples.\n",
    "\n",
    "    Args:\n",
    "        train_samples (list): List of training samples.\n",
    "        test_samples (list): List of test samples.\n",
    "        tokenizer (object): A preprocessing function that outputs a list of tokens.\n",
    "\n",
    "    Returns:\n",
    "        train_vectors, test_vectors: vectorized representations of the train and test sets, according to the BOW method.\n",
    "    \"\"\"\n",
    "\n",
    "    count_vectorizer = feature_extraction.text.CountVectorizer(tokenizer=tokenizer)\n",
    "\n",
    "    train_vectors = count_vectorizer.fit_transform(train_samples) # YOU FIT TO TRAIN DATA\n",
    "\n",
    "    test_vectors = count_vectorizer.transform(test_samples) # AND APPLY IT TO BOTH TEST AND TRAIN DATA\n",
    "\n",
    "    return train_vectors, test_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test this function and check the dimension of the resultant vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors, test_vectors = get_bow_representations(X_train, X_test, preprocess_text)\n",
    "print(train_vectors[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Considering the result of the previous cell, what is the number of unique words in the entire preprocessed train dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer here: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF implementation is similar to the Bag-of-Words one. But instead, we use the TfidfVectorizer (https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf_representations(train_samples, test_samples, tokenizer):\n",
    "    \"\"\"Returns a tf-idf based representation of both the train and test samples.\n",
    "\n",
    "    Args:\n",
    "        train_samples (list): List of training samples.\n",
    "        test_samples (list): List of test samples.\n",
    "        tokenizer (object): A preprocessing function that outputs a list of tokens.\n",
    "\n",
    "    Returns:\n",
    "        train_vectors, test_vectors: vectorized representations of the train and test sets, according to the BOW method.\n",
    "    \"\"\"\n",
    "\n",
    "    tfidf_vectorizer = feature_extraction.text.TfidfVectorizer(tokenizer=tokenizer)\n",
    "\n",
    "    train_vectors = tfidf_vectorizer.fit_transform(train_samples)\n",
    "\n",
    "    test_vectors = tfidf_vectorizer.transform(test_samples)\n",
    "\n",
    "    return train_vectors, test_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test this function, similarly to what you did earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors, test_vectors = get_tfidf_representations(X_train, X_test, preprocess_text)\n",
    "train_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you need a model to perform the main task: Text Classification of Clinical Descriptions. As so, we give you predictive functions that accept a model as input (You can choose your own classifier), train it upon training samples, and return a prediction against the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, train_samples, train_labels, test_samples):\n",
    "    \"\"\"Simple implementation of a Naive Bayes classifier.\n",
    "\n",
    "    Args:\n",
    "        train_samples (_type_): List of vectorized trained tweets.\n",
    "        train_labels (_type_): List of train labels.\n",
    "        test_samples (_type_): List of vectorized test tweets.\n",
    "\n",
    "    Returns:\n",
    "        preds: Predictions against the test set.\n",
    "    \"\"\"\n",
    "\n",
    "    model.fit(train_samples, train_labels)\n",
    "\n",
    "    return model.predict(test_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build your Pipeline!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to bring it all together! You got your preprocessing function, your feature extractors, and your training implentation. Now, you can combine them according to the structure of the traditional NLP pipeline you learned about in this first class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Start by getting your data in the correct format, i.e. a set of training tweets, a set of training labels, a set of test tweets, and a set of test labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This step was already performed when we splitted the data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Preprocess and get a numerical representation of the tweets. You should perform these two steps at once, since the vectorizers accept a preprocessing function as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize your text\n",
    "# Notice that you don't have to apply pre-processing first, because the vetorizers apply it themselves.\n",
    "# You must pass our preprocessing function to the feature extraction function, though.\n",
    "x_train_vectorized, x_test_vectorized = # INSERT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Get predictions against the test set, using a classifier of your choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose your fighter\n",
    "model = # INSERT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train your model and get predictions against the test samples\n",
    "preds = # INSERT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Check the performance you achieve through the selected pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your model in terms of accuracy, f1-score\n",
    "# REPLACE THE None's FOR THE RIGHT VARIABLES\n",
    "\n",
    "print(f\"Accuracy: {round(balanced_accuracy_score(None, None), 5)}\")\n",
    "print(f\"F1-score: {round(f1_score(None, None, average='macro'), 5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you managed to reach this cell without any issues, you probably saw that you can reach a quite reasonable results with this simple pipeline (for a classical 4-class problem, a random classifier would be right 25% of the times). Congratulations, you just built a decent text classifier capable of attributing a description to a certain medical specialty!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge: Note that this is a very simple task, since clinical note descriptions are already a summary of the most relevant traits of each clinical note! To achieve these results with the notes themselves would be much harder! Try to reproduce the pipeline you created, but use the column \"transcription\" of the original dataset as the X_data, instead."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "773824dfc984991d6e01ac3d420c6c1e8bda08505e3615b2d8fbbc5e78362c60"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

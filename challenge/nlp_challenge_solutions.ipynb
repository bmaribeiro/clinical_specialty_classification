{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge - Clinical Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You now know the structure of a Machine Learning pipeline, and how to implement it from the pre-processing of data to the evaluation of results. Good job! But can you do it with text data? In this challenge, you will have the opportunity to implement text processing techniques, including a simple strategy to represent text data in a structure that is compatible with Machine Learning algorithms: the Bag of Words model. Leveraging this strategy, and combining it with what you have learned by implementing the previous pipeline, you will be able to classify clinical notes according to the medical specialty where they were produced. Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\bruno.ribeiro\\anaconda3\\lib\\site-packages (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\bruno.ribeiro\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\bruno.ribeiro\\anaconda3\\lib\\site-packages (from scikit-learn) (1.21.5)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\bruno.ribeiro\\anaconda3\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\bruno.ribeiro\\anaconda3\\lib\\site-packages (from scikit-learn) (1.7.3)\n",
      "Requirement already satisfied: nltk in c:\\users\\bruno.ribeiro\\anaconda3\\lib\\site-packages (3.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\bruno.ribeiro\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\bruno.ribeiro\\anaconda3\\lib\\site-packages (from nltk) (2022.3.15)\n",
      "Requirement already satisfied: tqdm in c:\\users\\bruno.ribeiro\\anaconda3\\lib\\site-packages (from nltk) (4.64.0)\n",
      "Requirement already satisfied: click in c:\\users\\bruno.ribeiro\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\bruno.ribeiro\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\bruno.ribeiro\\anaconda3\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.20.3 in c:\\users\\bruno.ribeiro\\anaconda3\\lib\\site-packages (from pandas) (1.21.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\bruno.ribeiro\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\bruno.ribeiro\\anaconda3\\lib\\site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\bruno.ribeiro\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "!pip install scikit-learn\n",
    "!pip install nltk\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\bruno.ribeiro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import feature_extraction\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import balanced_accuracy_score, f1_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from collections import Counter\n",
    "from random import randint\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"nlp_challenge.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the data look? Here are a few examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>description</th>\n",
       "      <th>medical_specialty</th>\n",
       "      <th>sample_name</th>\n",
       "      <th>transcription</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12</td>\n",
       "      <td>Cerebral Angiogram - moyamoya disease.</td>\n",
       "      <td>Neurology</td>\n",
       "      <td>Moyamoya Disease</td>\n",
       "      <td>CC:, Confusion and slurred speech.,HX , (prima...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2755</td>\n",
       "      <td>EEG during wakefulness and light sleep is abn...</td>\n",
       "      <td>Neurology</td>\n",
       "      <td>Video EEG - 3</td>\n",
       "      <td>PROCEDURE: , EEG during wakefulness demonstrat...</td>\n",
       "      <td>neurology, epileptogenic, wakefulness, eeg, fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2756</td>\n",
       "      <td>A pleasant gentleman with a history of Wilson...</td>\n",
       "      <td>Neurology</td>\n",
       "      <td>Wilson's Disease - Letter</td>\n",
       "      <td>Doctor's Address,Dear Doctor:,This letter is a...</td>\n",
       "      <td>neurology, atrial enlargement, wilson's diseas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2757</td>\n",
       "      <td>This is a 43-year-old female with a history o...</td>\n",
       "      <td>Neurology</td>\n",
       "      <td>Video EEG</td>\n",
       "      <td>TIME SEEN: , 0734 hours and 1034 hours.,TOTAL ...</td>\n",
       "      <td>neurology, electroencephalography, eeg monitor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2758</td>\n",
       "      <td>The patient has a history of epilepsy and has...</td>\n",
       "      <td>Neurology</td>\n",
       "      <td>Video EEG - 1</td>\n",
       "      <td>DATE OF EXAMINATION: , Start:  12/29/2008 at 1...</td>\n",
       "      <td>neurology, non-epileptic events, temporal spik...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                        description  \\\n",
       "0          12             Cerebral Angiogram - moyamoya disease.   \n",
       "1        2755   EEG during wakefulness and light sleep is abn...   \n",
       "2        2756   A pleasant gentleman with a history of Wilson...   \n",
       "3        2757   This is a 43-year-old female with a history o...   \n",
       "4        2758   The patient has a history of epilepsy and has...   \n",
       "\n",
       "  medical_specialty                  sample_name  \\\n",
       "0         Neurology            Moyamoya Disease    \n",
       "1         Neurology               Video EEG - 3    \n",
       "2         Neurology   Wilson's Disease - Letter    \n",
       "3         Neurology                   Video EEG    \n",
       "4         Neurology               Video EEG - 1    \n",
       "\n",
       "                                       transcription  \\\n",
       "0  CC:, Confusion and slurred speech.,HX , (prima...   \n",
       "1  PROCEDURE: , EEG during wakefulness demonstrat...   \n",
       "2  Doctor's Address,Dear Doctor:,This letter is a...   \n",
       "3  TIME SEEN: , 0734 hours and 1034 hours.,TOTAL ...   \n",
       "4  DATE OF EXAMINATION: , Start:  12/29/2008 at 1...   \n",
       "\n",
       "                                            keywords  \n",
       "0                                                NaN  \n",
       "1  neurology, epileptogenic, wakefulness, eeg, fr...  \n",
       "2  neurology, atrial enlargement, wilson's diseas...  \n",
       "3  neurology, electroencephalography, eeg monitor...  \n",
       "4  neurology, non-epileptic events, temporal spik...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"description\" columns contains the samples of the data you will use to train your model. The target is the \"medical_specialty\" column. As you can see, these data is very different from time series! Start by isolating the information you need (i.e., create the variables X_data and y_data, referring to the description and medical_specialty columns, respectively)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data, y_data = data[\"description\"], data[\"medical_specialty\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, notice that your data is not split into train and test sets. Splitting data is one of the most defining steps in the Machine Learning pipeline. A bad split could result in data leakage, or a test set that does not accurately represent the true data distribution, which could consequently lead to over or under-estimation of results.\n",
    "\n",
    "To avoid falling into this pits, one must know the data one's working with. Let us check the class distributions. To do so, we will make use of the Counter from the collections library, which gives us the occurrences of each unique instance within a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({' Neurology': 223,\n",
       "         ' Radiology': 273,\n",
       "         ' Cardiovascular / Pulmonary': 372,\n",
       "         ' Gastroenterology': 230})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to make sure this distribution is similar both in training and testing. Apply a stratified method to do the split (https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idxs, test_idxs = next(StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=32).split(X_data, y_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = X_data[train_idxs], y_data[train_idxs], X_data[test_idxs], y_data[test_idxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the class distributions on both resultant sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({' Gastroenterology': 161,\n",
       "         ' Cardiovascular / Pulmonary': 260,\n",
       "         ' Radiology': 191,\n",
       "         ' Neurology': 156})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({' Cardiovascular / Pulmonary': 112,\n",
       "         ' Neurology': 67,\n",
       "         ' Radiology': 82,\n",
       "         ' Gastroenterology': 69})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get some words rolling! Here's a tutorial of text processing techniques: https://towardsdatascience.com/text-preprocessing-in-natural-language-processing-using-python-6113ff5decd8\n",
    "You should complement the function \"preprocess_text\", to return a list of clean tokens, when receiving a chunk of text. Note that the output of this function must be tokenized, to be compatible with the following steps of the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"Takes a text and returns the processed version of it.\n",
    "\n",
    "    Args:\n",
    "        text (str): raw text\n",
    "\n",
    "    Returns:\n",
    "        list: set of clean tokens containing the content of text\n",
    "    \"\"\"\n",
    "\n",
    "    # Normalize Text (lowercasing)\n",
    "    processed_text = text.lower()\n",
    "\n",
    "    # Add more preprocessing steps below:\n",
    "\n",
    "    # Try a few preprocessing techniques:\n",
    "    # - Tokenization\n",
    "    # - Stopwords Removal\n",
    "    # - Stemming/Lemmatization\n",
    "    processed_text = nltk.word_tokenize(processed_text)\n",
    "\n",
    "    # You can also implement other preprocessing you may find useful\n",
    "\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the function on a randomly sampled tweet from the train dataset (Give it a couple of tries to really see impact)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Note:  Neurologic examination sample.  \n",
      "Processed Note: ['neurologic', 'examination', 'sample', '.']\n"
     ]
    }
   ],
   "source": [
    "random_description = X_train.values[randint(0, X_train.shape[0])]\n",
    "print(f\"Original Note: {random_description}\")\n",
    "print(f\"Processed Note: {preprocess_text(random_description)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have cleaned every entry in our dataset, you can proceed to extract features. There are many ways to do this, but we will focus on two simple techniques: The bag-of-words model, and the tfidf. In the following cells, you'll be able to try several these two ways of vectorizing the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine Learning models do not accept strings as input. So how can we train one to perform text classification? We need to represent each text sample into a numeric vector. This numeric vector is our feature vector. In this challenge, you will implement two simple methods for text vectorization: The bag-of-words and the tfidf methodologies. Here's a quick introduction to both of them: https://machinelearningmastery.com/gentle-introduction-bag-words-model/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag-of-Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement a bag-of-words vectorization, we will use the CountVectorizer function from sklearn (https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Note: Notice that this function expects the output from the preprocessing function to be a tokenized tweet. If you did not implement a tokenizer yet, you must re-think your preprocessing methodology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bow_representations(train_samples, test_samples, tokenizer):\n",
    "    \"\"\"Returns a bag-of-words based representation of both the train and test samples.\n",
    "\n",
    "    Args:\n",
    "        train_samples (list): List of training samples.\n",
    "        test_samples (list): List of test samples.\n",
    "        tokenizer (object): A preprocessing function that outputs a list of tokens.\n",
    "\n",
    "    Returns:\n",
    "        train_vectors, test_vectors: vectorized representations of the train and test sets, according to the BOW method.\n",
    "    \"\"\"\n",
    "\n",
    "    count_vectorizer = feature_extraction.text.CountVectorizer(tokenizer=tokenizer)\n",
    "\n",
    "    train_vectors = count_vectorizer.fit_transform(train_samples)\n",
    "\n",
    "    test_vectors = count_vectorizer.transform(test_samples)\n",
    "\n",
    "    return train_vectors, test_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test this function and check the dimension of the resultant vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bruno.ribeiro\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2683)\n"
     ]
    }
   ],
   "source": [
    "train_vectors, test_vectors = get_bow_representations(X_train, X_test, preprocess_text)\n",
    "print(train_vectors[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Considering the result of the previous cell, what is the number of unique words in the entire preprocessed train dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer here: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF implementation is similar to the Bag-of-Words one. But instead, we use the TfidfVectorizer (https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf_representations(train_samples, test_samples, tokenizer):\n",
    "    \"\"\"Returns a tf-idf based representation of both the train and test samples.\n",
    "\n",
    "    Args:\n",
    "        train_samples (list): List of training samples.\n",
    "        test_samples (list): List of test samples.\n",
    "        tokenizer (object): A preprocessing function that outputs a list of tokens.\n",
    "\n",
    "    Returns:\n",
    "        train_vectors, test_vectors: vectorized representations of the train and test sets, according to the BOW method.\n",
    "    \"\"\"\n",
    "\n",
    "    tfidf_vectorizer = feature_extraction.text.TfidfVectorizer(tokenizer=tokenizer)\n",
    "\n",
    "    train_vectors = tfidf_vectorizer.fit_transform(train_samples)\n",
    "\n",
    "    test_vectors = tfidf_vectorizer.transform(test_samples)\n",
    "\n",
    "    return train_vectors, test_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test this function, similarly to what you did earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<768x2683 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 12946 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vectors, test_vectors = get_tfidf_representations(X_train, X_test, preprocess_text)\n",
    "train_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you need a model to perform the main task: Text Classification of Clinical Descriptions. As so, we give you predictive functions that accept a model as input (You can choose your own classifier), train it upon training samples, and return a prediction against the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, train_samples, train_labels, test_samples):\n",
    "    \"\"\"Simple implementation of a Naive Bayes classifier.\n",
    "\n",
    "    Args:\n",
    "        train_samples (_type_): List of vectorized trained tweets.\n",
    "        train_labels (_type_): List of train labels.\n",
    "        test_samples (_type_): List of vectorized test tweets.\n",
    "\n",
    "    Returns:\n",
    "        preds: Predictions against the test set.\n",
    "    \"\"\"\n",
    "\n",
    "    model.fit(train_samples, train_labels)\n",
    "\n",
    "    return model.predict(test_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build your Pipeline!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to bring it all together! You got your preprocessing function, your feature extractors, and your training implentation. Now, you can combine them according to the structure of the traditional NLP pipeline you learned about in this first class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Start by getting your data in the correct format, i.e. a set of training tweets, a set of training labels, a set of test tweets, and a set of test labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This step was already performed when we splitted the data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Preprocess and get a numerical representation of the tweets. You should perform these two steps at once, since the vectorizers accept a preprocessing function as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize your text\n",
    "# Notice that you don't have to apply pre-processing first, because the vetorizers apply it themselves.\n",
    "# You must pass our preprocessing function to the feature extraction function, though.\n",
    "x_train_vectorized, x_test_vectorized = get_bow_representations(X_train, X_test, preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Get predictions against the test set, using a classifier of your choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose your fighter\n",
    "model = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train your model and get predictions against the test samples\n",
    "preds = get_predictions(model, x_train_vectorized, y_train, x_test_vectorized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Check the performance you achieve through the selected pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6719\n",
      "F1-score: 0.67102\n"
     ]
    }
   ],
   "source": [
    "# Test your model in terms of accuracy, f1-score\n",
    "\n",
    "print(f\"Accuracy: {round(balanced_accuracy_score(y_test, preds), 5)}\")\n",
    "print(f\"F1-score: {round(f1_score(y_test, preds, average='macro'), 5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you managed to reach this cell without any issues, you probably saw that you can reach a quite reasonable results with this simple pipeline (for a classical 4-class problem, a random classifier would be right 25% of the times). Congratulations, you just built a decent text classifier capable of attributing a description to a certain medical specialty!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge: Note that this is a very simple task, since clinical note descriptions are already a summary of the most relevant traits of each clinical note! To achieve these results with the notes themselves would be much harder! Try to reproduce the pipeline you created, but use the column \"transcription\" of the original dataset as the X_data, instead."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "773824dfc984991d6e01ac3d420c6c1e8bda08505e3615b2d8fbbc5e78362c60"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
